{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff704263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 220 Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 2, 1/11/24, Concurrency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a1e92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Class size cannot be increased! \n",
    "- First HW to be published tomorrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6f0bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's Topics\n",
    "\n",
    " - Concurrency\n",
    "     - Threads and Processes\n",
    "     - I/O-Concurrency\n",
    "         - `threading`\n",
    "     - CPU-Concurrency (Parallelization)\n",
    "         - `multiprocessing` \n",
    "         - `Spark`\n",
    " \n",
    "### References\n",
    "- [SuperFastPython](https://superfastpython.com/thread-vs-process/) by Jason Brownlee\n",
    "- [An introduction to parallel programming](https://sebastianraschka.com/Articles/2014_multiprocessing.html) by Sebastian Raschka\n",
    "- [Speed Up Your Python Program With Concurrency](https://realpython.com/python-concurrency/) by Jim Anderson\n",
    "- [An Intro to Threading in Python](https://realpython.com/intro-to-python-threading/) by Jim Anderson\n",
    "- [3 Methods for Parallelization in Spark](https://towardsdatascience.com/3-methods-for-parallelization-in-spark-6a1a4333b473) by Ben Weber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44dc81e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Threads and Processes\n",
    "\n",
    "A computer can have multiple CPUs, each CPU has multiple cores (e.g., two quad-core CPUs). \n",
    "All the CPUs are connected to memory (e.g., 64G memory). \n",
    "CPU cores can execute in parallel. \n",
    "\n",
    "<div>\n",
    "<img src=\"../images/fig1.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58803ce1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A __process__ is the *operating system’s spawned and controlled entity that encapsulates an executing application* ([Breshears: The Art of Concurrency](https://amzn.to/3J74TRr)). \n",
    "\n",
    "A __thread__ is a path of execution which belongs to a process. \n",
    "\n",
    "Each thread belongs to a process. In single-threaded processes, the process contains one thread. In multithreaded processes, the process contains more than one thread, and the process is accomplishing a number of things at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed164f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Threads can share memory within a process. This means that functions executed in new threads can access the same data and state. These might be global variables or data shared via function arguments. As such, sharing state between threads is straightforward.\n",
    "\n",
    "Threads are sometimes called lightweight processes because they have their own stack but can access shared data. \n",
    "\n",
    "<div>\n",
    "<img src=\"../images/fig3.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f70e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the other hand, processes are 'share nothing', i.e., they independently execute without sharing memory or state. This makes it easier to turn into a distributed application, but typically, sharing data between processes requires explicit mechanisms.\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/fig4.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0677f060",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Python allows to execute code using the principle of global interpreter lock (GIL). This means that only one thread can be executed at a time. This simplifies implementation, but makes it more difficult to execute code concurrently. \n",
    "\n",
    "Today, we will explore the advantages of executing multiple processes and threads and discuss under what circumstances which approach is most adequate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352750d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are two major kind of tasks, that will slow down your program: CPU-bound and I/O-bound.\n",
    "\n",
    "I/O-bound tasks cause your program to slow down because it frequently must wait for input/output (I/O) from some external resource. They arise when your program interacts with other sources, i.e., when your are *requesting* data from another source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acafebe",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://files.realpython.com/media/IOBound.4810a888b457.png style=\"width: 1700px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d51c2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CPU-bound tasks are those that require a lot of *computational* effort to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792b789",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://files.realpython.com/media/CPUBound.d2d32cb2626c.png style=\"width: 1700px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc078db",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will use threads and the module `threading` for I/O-bound tasks and processes and the module `multiprocessing` for CPU-bound tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b8610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### I/O-Concurrency\n",
    "\n",
    "We’ll start with a non-concurrent version of this I/O-bound task. Namely, we will use `request` to request data from a [website](https://anson.ucdavis.edu/~kramling/). For `requests.Session`, see [here](https://requests.readthedocs.io/en/latest/user/advanced/) and [docs](https://requests.readthedocs.io/en/latest/api/?#requests.Session). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a93301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests, time\n",
    "\n",
    "def download_site(url, session):\n",
    "    session.get(url) # fetch information from url \n",
    "    # ... do something ... \n",
    "    \n",
    "def download_all_sites(sites):\n",
    "    session = requests.Session()\n",
    "    [download_site(url, session) for url in sites]\n",
    "        \n",
    "def task(): \n",
    "    sites = [\"https://anson.ucdavis.edu/~kramling/\"] * 80\n",
    "    start_time = time.time()\n",
    "    download_all_sites(sites)\n",
    "    print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3bbfd47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.359849214553833\n"
     ]
    }
   ],
   "source": [
    "task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4079ebd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will now use concurrent threads that accomplish the same task, retrieving information by executing `requests.get` more efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "284bab80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures, threading\n",
    "\n",
    "thread_local = threading.local() # instantiates thread to create local data (here: session-attr.)\n",
    "\n",
    "def download_site(url):\n",
    "    session = get_session()\n",
    "    session.get(url)\n",
    "    # ... do something ... \n",
    "\n",
    "def download_all_sites(sites):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 12) as executor:\n",
    "        executor.map(download_site, sites)\n",
    "\n",
    "def get_session():\n",
    "    '''Create a new requests.Session if there is none in thread_local'''\n",
    "    if not hasattr(thread_local, \"session\"): \n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a96540c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0996081829071045\n"
     ]
    }
   ],
   "source": [
    "task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22615b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have created a `concurrent.futures.ThreadPoolExecutor`. It creates five threads that are run concurrently. \n",
    "\n",
    "Also, each thread will become its own separate `requests.Session`. This is one of the interesting and difficult issues with threading. Because the operating system is in control of switching between threads, any data that is shared between the threads needs to be protected, or thread-safe. Unfortunately `requests.Session` is not thread-safe. If untreated, *race conditions* can produce hard-to-detect bugs. \n",
    "\n",
    "Here, we use `threading.local()` to instantiate an object that looks like a global but is specific to each individual thread. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25bd665",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The code above is faster than the non-concurrent version, because the I/O-bound has been circumvented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b31dc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://files.realpython.com/media/Threading.3eef48da829e.png style=\"width: 1600px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a57dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CPU-Concurrency (Parallelization)\n",
    "\n",
    "The above example of concurrency run only on a single CPU. This is due to the GIL. The `multiprocessing` module breaks down that barrier and runs code across multiple CPUs. \n",
    "\n",
    "It does this by creating a new instance of the Python interpreter (a new process) to run on each CPU and then farming out part of your program to run on it.  Bringing up a separate Python interpreter is not as fast as starting a new thread in the current Python interpreter. Regarding `.set_start_method`, see [here](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "801a8412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9bebb16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3907041549682617\n"
     ]
    }
   ],
   "source": [
    "session = None\n",
    "multiprocessing.set_start_method(\"fork\", True) # new process will be a copy from previous process\n",
    "\n",
    "def set_global_session():\n",
    "    global session\n",
    "    if not session:\n",
    "        session = requests.Session()\n",
    "\n",
    "def download_site(url):\n",
    "    session.get(url)\n",
    "    # ... do something ... \n",
    "\n",
    "def download_all_sites(sites):\n",
    "    with multiprocessing.Pool(initializer=set_global_session, processes = 8) as pool: # change processes!\n",
    "        pool.map(download_site, sites)\n",
    "    \n",
    "task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503bc19",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We may even be faster than with `threading`, since we are running up to 8 processes in parallel, but `multiprocessing` cannot scale beyond the number of cores in your local machine. \n",
    "The default argument `processes` of `multiprocessing.pool.Pool` ([docs](https://docs.python.org/3/library/multiprocessing.html?#module-multiprocessing.pool)), is the number of available cores on the machine. \n",
    "\n",
    "Remember that each process has its own memory space (share-nothing). That means that they cannot share things like a `requests.Session` object. We don’t want to create a new Session each time the function is called, you want to create one for each process, which calls `request.get` multiple times after another. \n",
    "\n",
    "The `initializer` function parameter is built for just this case. We initialize a global `session` variable to hold the single `requests.Session` *for each process*. Because each process has its own memory space, the global for each one will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e2832",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<img src=https://files.realpython.com/media/MProc.7cf3be371bbc.png style=\"width: 1600px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05368937",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This course will deal with fetching information from the web, usually via requests. While these requests will usually be bound by third partys who maintain the servers we are requesting from as well, they are in essence I/O-bound task. `multiprocessing` however is useful for CPU-bound tasks. Consequently, lets consider a computational problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef6c4a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the purposes of our example, we’ll use a somewhat silly function to create something that takes a long time to run on the CPU. This function computes the sum of the squares of each number from 0 to the passed-in value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1406ac99",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def problem(number):\n",
    "    return sum(i * i for i in range(number))\n",
    "\n",
    "def find_sums(numbers):\n",
    "    for number in numbers:\n",
    "        problem(number)\n",
    "\n",
    "def task():\n",
    "    numbers = [5_000_000 + x for x in range(20)]\n",
    "    start_time = time.time()\n",
    "    find_sums(numbers)\n",
    "    print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c9ef724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.062252759933472\n"
     ]
    }
   ],
   "source": [
    "task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d247d32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This code calls `find_sums` 20 times with a different large number each time. It does all of this on a single thread in a single process on a single CPU. The execution timing diagram looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c315448",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://files.realpython.com/media/CPUBound.d2d32cb2626c.png style=\"width: 1600px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55b3f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since there is no I/O waiting time, `threading` will not speed up this problem. We can however speed the computation by using our multiple cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a87f2626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiprocessing.set_start_method(\"fork\", True) # has already been set\n",
    "def find_sums(numbers):\n",
    "    pool = multiprocessing.Pool()\n",
    "    return pool.map(problem, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d83043ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6652929782867432\n"
     ]
    }
   ],
   "source": [
    "task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceaa426",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This code is similar as for the I/O-bound problem, but here you don’t need to worry about the `requests.Session` object. Notably, the speed-up is not equal to the number of cores, as each process has to set up its own Python interpreter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72c9cc",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://files.realpython.com/media/CPUMP.69c1a7fad9c4.png style=\"width: 1600px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1facef36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While this codes is easy and fast, all the single processes are automatically taken care of with `multiprocessing.Pool`. The results returned by `find_sums` are gathered by `multiprocessing.map` as a <kbd>list</kbd> type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c97ec85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 5]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sums(range(0, 4)) # returns a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cea47",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, many solutions require communication between the processes. This can add some complexity to your solution that a non-concurrent program would not need to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248b939",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A `multiprocessing.Queue` object provides a mechanism to pass data between a parent process and the descendent processes of it. It adheres the *first in first out* principle. We can retrieve with the `multiprocessing.get` method and set with the `multiprocessing.put` method. \n",
    "\n",
    "<div>\n",
    "<img src=https://media.geeksforgeeks.org/wp-content/uploads/multiprocessing-python-4.png style=\"width: 1400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1d2430a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "q = multiprocessing.Queue()\n",
    "def myfun(q, i): \n",
    "    q.put(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4c28dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Process name='Process-136' parent=38841 initial>,\n",
       " <Process name='Process-137' parent=38841 initial>,\n",
       " <Process name='Process-138' parent=38841 initial>,\n",
       " <Process name='Process-139' parent=38841 initial>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize process\n",
    "processes = [multiprocessing.Process(target=myfun, args = (q, i)) for i in range(4)]\n",
    "processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db128bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter            38841   0.0  0.7 35093992  61424   ??  Ss    8:23AM   0:22.11 /Users/peter/opt/anaconda3/bin/python -m ipykernel_launcher -f /Users/peter/Library/Jupyter/runtime/kernel-c9f1c358-8ec9-44ab-93e8-7a15f9fdd64d.json\r\n"
     ]
    }
   ],
   "source": [
    "!ps aux | grep \"[3]8841\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ce64c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter            38841   5.3  0.7 35093992  61500   ??  Ss    8:23AM   0:22.08 /Users/peter/opt/anaconda3/bin/python -m ipykernel_launcher -f /Users/peter/Library/Jupyter/runtime/kernel-c9f1c358-8ec9-44ab-93e8-7a15f9fdd64d.json\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "833f44bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The processess are initialized but not yet executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d332f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run processes\n",
    "for process in processes:\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a71474cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join processes\n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b4480",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main purpose of `multiprocessing.join` ([docs](https://docs.python.org/3/library/multiprocessing.html?#multiprocessing.Process.join)) is to ensure that a child process has completed before the main process does anything that depends on the work of the child process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a834ede7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Process name='Process-136' pid=39587 parent=38841 stopped exitcode=0>,\n",
       " <Process name='Process-137' pid=39588 parent=38841 stopped exitcode=0>,\n",
       " <Process name='Process-138' pid=39589 parent=38841 stopped exitcode=0>,\n",
       " <Process name='Process-139' pid=39590 parent=38841 stopped exitcode=0>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9464f3",
   "metadata": {},
   "source": [
    "The value `exitcode=0` means that the process has been completed successfully, without error ([docs](https://docs.python.org/3/library/multiprocessing.html?#multiprocessing.Process.exitcode)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d496f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d14c5659",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<multiprocessing.queues.Queue at 0x7f8c180180a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85f30a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q.get() for process in processes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9047275e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.empty()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77178fd1",
   "metadata": {},
   "source": [
    "While `multiprocessing` allows you to steer the processes directly, many statistical problems are already implemented and ready for parallel computing, e.g. via `Spark`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b509a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark\n",
    "\n",
    "Apache Spark is a computational engine that works with huge sets of data by processing them in parallel and batch systems. Spark is written in Scala, and [PySpark](https://www.dominodatalab.com/data-science-dictionary/pyspark) was released to support the collaboration of Spark and Python. \n",
    "\n",
    "The key data type used in PySpark is the Spark dataframe. This object can be thought of as a table distributed across a cluster, and has functionality that is similar to dataframes in `pandas`. If you want to do distributed computation using `PySpark`, then you’ll need to perform operations on Spark dataframes and not other Python data types.\n",
    "\n",
    "Here we explore how to perform tasks using `PySpark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e677095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/peter/opt/anaconda3/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/peter/opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a93ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We consider a simple regression model for predicting house prices. Lets consider the non-serialized version first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca27e6fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "165dd157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the california housing data set\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# convert to a Pandas Data Frame\n",
    "housing_pd = pd.DataFrame(data= np.c_[housing['data'],housing['target']], \n",
    "                          columns= np.append(housing['feature_names'], 'target')).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7274f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "699c21de",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17600</th>\n",
       "      <td>5.0360</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.256318</td>\n",
       "      <td>0.971119</td>\n",
       "      <td>582.0</td>\n",
       "      <td>2.101083</td>\n",
       "      <td>37.30</td>\n",
       "      <td>-121.90</td>\n",
       "      <td>2.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11544</th>\n",
       "      <td>7.0879</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.969697</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>878.0</td>\n",
       "      <td>2.956229</td>\n",
       "      <td>33.76</td>\n",
       "      <td>-118.04</td>\n",
       "      <td>3.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14990</th>\n",
       "      <td>3.7763</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.686275</td>\n",
       "      <td>1.033613</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>3.198880</td>\n",
       "      <td>32.72</td>\n",
       "      <td>-117.02</td>\n",
       "      <td>1.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12865</th>\n",
       "      <td>3.7031</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.945619</td>\n",
       "      <td>1.099698</td>\n",
       "      <td>996.0</td>\n",
       "      <td>3.009063</td>\n",
       "      <td>38.69</td>\n",
       "      <td>-121.34</td>\n",
       "      <td>1.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2.2466</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.277259</td>\n",
       "      <td>0.971963</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>3.140187</td>\n",
       "      <td>36.64</td>\n",
       "      <td>-119.82</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "17600  5.0360      52.0  5.256318   0.971119       582.0  2.101083     37.30   \n",
       "11544  7.0879      16.0  6.969697   0.885522       878.0  2.956229     33.76   \n",
       "14990  3.7763      36.0  5.686275   1.033613      1142.0  3.198880     32.72   \n",
       "12865  3.7031      17.0  5.945619   1.099698       996.0  3.009063     38.69   \n",
       "2066   2.2466      30.0  5.277259   0.971963      1008.0  3.140187     36.64   \n",
       "\n",
       "       Longitude  target  \n",
       "17600    -121.90   2.966  \n",
       "11544    -118.04   3.388  \n",
       "14990    -117.02   1.269  \n",
       "12865    -121.34   1.143  \n",
       "2066     -119.82   0.960  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adc23d69",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/11 10:12:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/11 10:13:03 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36d44ab6",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peter/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/peter/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# convert to a Spark data frame\n",
    "housing_sp = spark.createDataFrame(housing_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7cddd49",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "features = housing_sp.schema.names[:]\n",
    "target = features.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b49c77cb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# convert to vector representation for MLlib\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\" )\n",
    "housing = assembler.transform(housing_sp).select('features', 'target') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce0d1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "See [docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0753108",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# linear regresion with Spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a18276e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# linear regression with penalization\n",
    "lr = LinearRegression(labelCol=\"target\", featuresCol=\"features\", \n",
    "                      elasticNetParam = 1.0, # lasso / l1-penalization\n",
    "                      standardization = False, \n",
    "                      fitIntercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d0020ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lrparamGrid = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.001, 0.01, 0.1, 0.5, 1.0, 2.0])\n",
    "               .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fccffb11",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "lrevaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                  labelCol=\"target\", metricName=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bbc9bbe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "lrcv = CrossValidator(estimator = lr,\n",
    "                      estimatorParamMaps = lrparamGrid,\n",
    "                      evaluator = lrevaluator,\n",
    "                      numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "457ad1c1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/11 10:15:28 WARN TaskSetManager: Stage 0 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/11 10:15:29 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/01/11 10:15:29 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/01/11 10:15:29 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/11 10:15:29 WARN TaskSetManager: Stage 1 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:29 WARN TaskSetManager: Stage 2 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:29 WARN TaskSetManager: Stage 3 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:29 WARN TaskSetManager: Stage 4 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:29 WARN TaskSetManager: Stage 5 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 6 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 7 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 8 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 9 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 10 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 11 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 12 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 13 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 14 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 15 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 16 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 17 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 18 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:30 WARN TaskSetManager: Stage 19 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 20 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 21 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 22 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 23 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 24 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 25 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 26 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 27 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 28 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 29 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 30 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 31 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 32 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 33 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 34 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 35 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 36 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:31 WARN TaskSetManager: Stage 37 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 38 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 39 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 40 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 41 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 42 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 43 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 44 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 45 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 46 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 47 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 48 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 49 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 50 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 51 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 52 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 53 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 54 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:32 WARN TaskSetManager: Stage 55 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 56 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 57 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 58 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 59 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 60 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 61 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 62 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 63 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 64 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 65 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 66 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 67 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 68 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 69 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 70 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 71 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 72 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 73 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:33 WARN TaskSetManager: Stage 74 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 75 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 76 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 77 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 78 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 79 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 80 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 81 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 82 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 83 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 84 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 85 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 86 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 87 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 88 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 89 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 90 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 10:15:34 WARN TaskSetManager: Stage 91 contains a task of very large size (1698 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Run cross validations\n",
    "lrcvModel = lrcv.fit(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc51b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "See [docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegressionTrainingSummary.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "640c26fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.623165451536651"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Model Summary Statistics\n",
    "lrcvSummary = lrcvModel.bestModel.summary\n",
    "lrcvSummary.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1231f009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.4334, 0.0179, -0.0691, 0.2993, 0.0, -0.0049, -0.0349, -0.0094])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcvModel.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a05c85cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'target']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_sp.schema.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c15a9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary \n",
    "\n",
    "- There are I/O- and CPU-bound problems\n",
    "- Use `threading` for I/O-bound problems, `multiprocessing` for CPU-bound problems\n",
    "- Communication between processes is cumbersome\n",
    "- For many CPU-bound tasks, there may be implemented solutions. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "progress": true,
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
